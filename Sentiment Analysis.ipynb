{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be39b0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.keras import models, layers, optimizers\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import bz2\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71cde4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adcbea4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1]\n",
      "['Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^', \"The best soundtrack ever to anything.: I'm reading a lot of reviews saying that this is the best 'game soundtrack' and I figured that I'd write a review to disagree a bit. This in my opinino is Yasunori Mitsuda's ultimate masterpiece. The music is timeless and I'm been listening to it for years now and its beauty simply refuses to fade.The price tag on this is pretty staggering I must say, but if you are going to buy any cd for this much money, this is the only one that I feel would be worth every penny.\", 'Amazing!: This soundtrack is my favorite music of all time, hands down. The intense sadness of \"Prisoners of Fate\" (which means all the more if you\\'ve played the game) and the hope in \"A Distant Promise\" and \"Girl who Stole the Star\" have been an important inspiration to me personally throughout my teen years. The higher energy tracks like \"Chrono Cross ~ Time\\'s Scar~\", \"Time of the Dreamwatch\", and \"Chronomantique\" (indefinably remeniscent of Chrono Trigger) are all absolutely superb as well.This soundtrack is amazing music, probably the best of this composer\\'s work (I haven\\'t heard the Xenogears soundtrack, so I can\\'t say for sure), and even if you\\'ve never played the game, it would be worth twice the price to buy it.I wish I could give it 6 stars.', \"Excellent Soundtrack: I truly like this soundtrack and I enjoy video game music. I have played this game and most of the music on here I enjoy and it's truly relaxing and peaceful.On disk one. my favorites are Scars Of Time, Between Life and Death, Forest Of Illusion, Fortress of Ancient Dragons, Lost Fragment, and Drowned Valley.Disk Two: The Draggons, Galdorb - Home, Chronomantique, Prisoners of Fate, Gale, and my girlfriend likes ZelbessDisk Three: The best of the three. Garden Of God, Chronopolis, Fates, Jellyfish sea, Burning Orphange, Dragon's Prayer, Tower Of Stars, Dragon God, and Radical Dreamers - Unstealable Jewel.Overall, this is a excellent soundtrack and should be brought by those that like video game music.Xander Cross\", \"Remember, Pull Your Jaw Off The Floor After Hearing it: If you've played the game, you know how divine the music is! Every single song tells a story of the game, it's that good! The greatest songs are without a doubt, Chrono Cross: Time's Scar, Magical Dreamers: The Wind, The Stars, and the Sea and Radical Dreamers: Unstolen Jewel. (Translation varies) This music is perfect if you ask me, the best it can be. Yasunori Mitsuda just poured his heart on and wrote it down on paper.\"]\n"
     ]
    }
   ],
   "source": [
    "# Define a function to load labels and text from the dataset\n",
    "\n",
    "def get_labels_and_texts(file):\n",
    "def get_labels_and_texts(file):\n",
    "    labels = []\n",
    "    texts = []\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            labels.append(int(line[9]) - 1)\n",
    "            texts.append(line[10:].strip())\n",
    "    return np.array(labels), texts\n",
    "\n",
    "# Load data from the extracted .txt files\n",
    "train_labels, train_texts = get_labels_and_texts('train.ft.txt')\n",
    "test_labels, test_texts = get_labels_and_texts('test.ft.txt')\n",
    "\n",
    "# Check the first few labels and texts\n",
    "print(train_labels[:5])\n",
    "print(train_texts[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eac6f082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text normalization: cleaning the text data by removing non-alphanumeric characters\n",
    "\n",
    "import re\n",
    "NON_ALPHANUM = re.compile(r'[\\W]')\n",
    "NON_ASCII = re.compile(r'[^a-z0-1\\s]')\n",
    "\n",
    "def normalize_texts(texts):\n",
    "    normalized_texts = []\n",
    "    for text in texts:\n",
    "        lower = text.lower()\n",
    "        no_punctuation = NON_ALPHANUM.sub(r' ', lower)\n",
    "        no_non_ascii = NON_ASCII.sub(r'', no_punctuation)\n",
    "        normalized_texts.append(no_non_ascii)\n",
    "    return normalized_texts\n",
    "\n",
    "# Apply text normalization to the training and testing data\n",
    "train_texts = normalize_texts(train_texts)\n",
    "test_texts = normalize_texts(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81fb1934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training data into training and validation sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, random_state=57643892, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c15825e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization: Convert text data into numeric sequences for model input\n",
    "MAX_FEATURES = 12000\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "train_texts = tokenizer.texts_to_sequences(train_texts)\n",
    "val_texts = tokenizer.texts_to_sequences(val_texts)\n",
    "test_texts = tokenizer.texts_to_sequences(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36542526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding: Ensure that all sequences are of the same length\n",
    "\n",
    "MAX_LENGTH = max(len(train_ex) for train_ex in train_texts)\n",
    "train_texts = pad_sequences(train_texts, maxlen=MAX_LENGTH)\n",
    "val_texts = pad_sequences(val_texts, maxlen=MAX_LENGTH)\n",
    "test_texts = pad_sequences(test_texts, maxlen=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2e841b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers\n",
    "\n",
    "# Building a CNN model for text classification\n",
    "def build_model():\n",
    "    # Input layer for the sequences (word indexes)\n",
    "    sequences = layers.Input(shape=(MAX_LENGTH,))\n",
    "    # Embedding layer to map word indexes into dense vectors\n",
    "    embedded = layers.Embedding(MAX_FEATURES, 64)(sequences)\n",
    "    \n",
    "    # First convolution layer followed by batch normalization and max-pooling\n",
    "    x = layers.Conv1D(64, 3, activation='relu')(embedded)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPool1D(3)(x)\n",
    "    \n",
    "    # Second convolution layer with larger kernel size\n",
    "    x = layers.Conv1D(64, 5, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPool1D(5)(x)\n",
    "    \n",
    "    # Final convolution layer followed by global pooling\n",
    "    x = layers.Conv1D(64, 5, activation='relu')(x)\n",
    "    x = layers.GlobalMaxPool1D()(x)\n",
    "    \n",
    "    # Flattening the pooled output and adding dense layers for final classification\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(100, activation='relu')(x)\n",
    "    \n",
    "    # Output layer for binary classification with sigmoid activation\n",
    "    predictions = layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    # Compile the model with optimizer, loss function, and evaluation metric\n",
    "    model = models.Model(inputs=sequences, outputs=predictions)\n",
    "    model.compile(\n",
    "        optimizer='rmsprop',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['binary_accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Build and compile the CNN model\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83c67667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m22500/22500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1148s\u001b[0m 51ms/step - binary_accuracy: 0.9215 - loss: 0.1968 - val_binary_accuracy: 0.9457 - val_loss: 0.1470\n",
      "Epoch 2/2\n",
      "\u001b[1m22500/22500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1245s\u001b[0m 55ms/step - binary_accuracy: 0.9485 - loss: 0.1403 - val_binary_accuracy: 0.9492 - val_loss: 0.1391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3dfcbb150>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model on the training data and validate on validation data\n",
    "model.fit(\n",
    "    train_texts, \n",
    "    train_labels, \n",
    "    batch_size=128,\n",
    "    epochs=2,\n",
    "    validation_data=(val_texts, val_labels), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1415d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 4ms/step\n",
      "Accuracy score: 0.9488\n",
      "F1 score: 0.9487\n",
      "ROC AUC score: 0.9874\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using accuracy, F1 score, and ROC AUC\n",
    "preds = model.predict(test_texts)\n",
    "print('Accuracy score: {:0.4}'.format(accuracy_score(test_labels, 1 * (preds > 0.5))))\n",
    "print('F1 score: {:0.4}'.format(f1_score(test_labels, 1 * (preds > 0.5))))\n",
    "print('ROC AUC score: {:0.4}'.format(roc_auc_score(test_labels, preds)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
